{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ji/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "#Imports for plotting\n",
    "from ast import literal_eval \n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt   \n",
    "from matplotlib import cm\n",
    "import matplotlib_venn as venn\n",
    "from matplotlib_venn import venn2, venn2_circles, venn3, venn3_circles\n",
    "%matplotlib inline\n",
    "\n",
    "#options\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.options.display.max_rows = 50\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "#NLP Packages\n",
    "from textwrap import wrap\n",
    "from textblob import TextBlob \n",
    "from wordcloud import WordCloud, ImageColorGenerator \n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.probability import FreqDist \n",
    "from nltk import RegexpTokenizer, PorterStemmer, pos_tag  \n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "import string\n",
    "import re  \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #WBOT\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')  \n",
    "\n",
    "# sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, f1_score, recall_score, confusion_matrix\n",
    "\n",
    "# xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# utilities\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# preferences\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating_overall</th>\n",
       "      <th>employee_title_title_known</th>\n",
       "      <th>employee_title_title_unknown</th>\n",
       "      <th>employee_status_current_employee</th>\n",
       "      <th>employee_status_former_employee</th>\n",
       "      <th>employee_status_unknown_status</th>\n",
       "      <th>helpful_helpful</th>\n",
       "      <th>helpful_not_helpful</th>\n",
       "      <th>recommends_does_not_recommend</th>\n",
       "      <th>recommends_recommends</th>\n",
       "      <th>recommends_unknown_recommends</th>\n",
       "      <th>area_central</th>\n",
       "      <th>area_east_coast</th>\n",
       "      <th>area_mountain</th>\n",
       "      <th>area_outside_US</th>\n",
       "      <th>area_unknown</th>\n",
       "      <th>area_west_coast</th>\n",
       "      <th>company_outlook_negative_outlook</th>\n",
       "      <th>company_outlook_neutral_outlook</th>\n",
       "      <th>company_outlook_outlook_unknown</th>\n",
       "      <th>company_outlook_positive_outlook</th>\n",
       "      <th>opinion_of_CEO_approves_of_CEO</th>\n",
       "      <th>opinion_of_CEO_disapproves_of_CEO</th>\n",
       "      <th>opinion_of_CEO_no_opinion_of_CEO</th>\n",
       "      <th>opinion_of_CEO_unknown_opinion_of_CEO</th>\n",
       "      <th>review_text</th>\n",
       "      <th>orig_text</th>\n",
       "      <th>lem</th>\n",
       "      <th>stem</th>\n",
       "      <th>pos</th>\n",
       "      <th>noun</th>\n",
       "      <th>adj</th>\n",
       "      <th>adv</th>\n",
       "      <th>verb</th>\n",
       "      <th>characters</th>\n",
       "      <th>tokens</th>\n",
       "      <th>words</th>\n",
       "      <th>sentences</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>stem_str</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[great, always, room, improvement, everybody, works, together, cons, love, job, helpfu]</td>\n",
       "      <td>great always room for improvement. everybody works together.  no cons i love my job Helpfu nan</td>\n",
       "      <td>[great, always, room, improvement, everybody, work, together, con, love, job, helpfu]</td>\n",
       "      <td>[great, alway, room, improv, everybodi, work, togeth, con, love, job, helpfu]</td>\n",
       "      <td>[JJ, RB, NN, NN, NN, VBZ, RB, NNS, VBP, NN, NN]</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>94</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>5.529412</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>great alway room improv everybodi work togeth con love job helpfu</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating_overall  employee_title_title_known  employee_title_title_unknown  \\\n",
       "0             5.0                           1                             0   \n",
       "\n",
       "   employee_status_current_employee  employee_status_former_employee  \\\n",
       "0                                 1                                0   \n",
       "\n",
       "   employee_status_unknown_status  helpful_helpful  helpful_not_helpful  \\\n",
       "0                               0                0                    1   \n",
       "\n",
       "   recommends_does_not_recommend  recommends_recommends  \\\n",
       "0                              0                      1   \n",
       "\n",
       "   recommends_unknown_recommends  area_central  area_east_coast  \\\n",
       "0                              0             0                1   \n",
       "\n",
       "   area_mountain  area_outside_US  area_unknown  area_west_coast  \\\n",
       "0              0                0             0                0   \n",
       "\n",
       "   company_outlook_negative_outlook  company_outlook_neutral_outlook  \\\n",
       "0                                 0                                0   \n",
       "\n",
       "   company_outlook_outlook_unknown  company_outlook_positive_outlook  \\\n",
       "0                                0                                 1   \n",
       "\n",
       "   opinion_of_CEO_approves_of_CEO  opinion_of_CEO_disapproves_of_CEO  \\\n",
       "0                               0                                  0   \n",
       "\n",
       "   opinion_of_CEO_no_opinion_of_CEO  opinion_of_CEO_unknown_opinion_of_CEO  \\\n",
       "0                                 1                                      0   \n",
       "\n",
       "                                                                               review_text  \\\n",
       "0  [great, always, room, improvement, everybody, works, together, cons, love, job, helpfu]   \n",
       "\n",
       "                                                                                        orig_text  \\\n",
       "0  great always room for improvement. everybody works together.  no cons i love my job Helpfu nan   \n",
       "\n",
       "                                                                                     lem  \\\n",
       "0  [great, always, room, improvement, everybody, work, together, con, love, job, helpfu]   \n",
       "\n",
       "                                                                            stem  \\\n",
       "0  [great, alway, room, improv, everybodi, work, togeth, con, love, job, helpfu]   \n",
       "\n",
       "                                               pos      noun       adj  \\\n",
       "0  [JJ, RB, NN, NN, NN, VBZ, RB, NNS, VBP, NN, NN]  0.545455  0.090909   \n",
       "\n",
       "        adv      verb  characters  tokens  words  sentences  avg_word_len  \\\n",
       "0  0.181818  0.181818          94      11     17          3      5.529412   \n",
       "\n",
       "   avg_sent_len  \\\n",
       "0      5.666667   \n",
       "\n",
       "                                                            stem_str  \\\n",
       "0  great alway room improv everybodi work togeth con love job helpfu   \n",
       "\n",
       "   sentiment  \n",
       "0       0.65  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data\n",
    "df = pd.read_pickle('df_processed_full.pickle')\n",
    "\n",
    "#Preview dataset\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rating_overall', 'employee_title_title_known',\n",
       "       'employee_title_title_unknown', 'employee_status_current_employee',\n",
       "       'employee_status_former_employee', 'employee_status_unknown_status',\n",
       "       'helpful_helpful', 'helpful_not_helpful',\n",
       "       'recommends_does_not_recommend', 'recommends_recommends',\n",
       "       'recommends_unknown_recommends', 'area_central', 'area_east_coast',\n",
       "       'area_mountain', 'area_outside_US', 'area_unknown', 'area_west_coast',\n",
       "       'company_outlook_negative_outlook', 'company_outlook_neutral_outlook',\n",
       "       'company_outlook_outlook_unknown', 'company_outlook_positive_outlook',\n",
       "       'opinion_of_CEO_approves_of_CEO', 'opinion_of_CEO_disapproves_of_CEO',\n",
       "       'opinion_of_CEO_no_opinion_of_CEO',\n",
       "       'opinion_of_CEO_unknown_opinion_of_CEO', 'review_text', 'orig_text',\n",
       "       'lem', 'stem', 'pos', 'noun', 'adj', 'adv', 'verb', 'characters',\n",
       "       'tokens', 'words', 'sentences', 'avg_word_len', 'avg_sent_len',\n",
       "       'stem_str', 'sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed = ['rating_overall', 'employee_title_title_known','employee_title_title_unknown', 'employee_status_current_employee', \n",
    "          'employee_status_former_employee', 'employee_status_unknown_status', 'helpful_helpful', 'helpful_not_helpful','recommends_does_not_recommend',\n",
    "          'recommends_recommends','recommends_unknown_recommends', 'area_central', 'area_east_coast','area_mountain', 'area_outside_US', \n",
    "          'area_unknown', 'area_west_coast','company_outlook_negative_outlook', 'company_outlook_neutral_outlook',\n",
    "          'company_outlook_outlook_unknown', 'company_outlook_positive_outlook', 'opinion_of_CEO_approves_of_CEO', \n",
    "          'opinion_of_CEO_disapproves_of_CEO', 'opinion_of_CEO_no_opinion_of_CEO','opinion_of_CEO_unknown_opinion_of_CEO', \n",
    "           'noun', 'adj', 'adv', 'verb', 'characters','tokens', 'words', 'sentences', 'avg_word_len', 'avg_sent_len', 'stem_str', 'sentiment']\n",
    "\n",
    "dfa = df[needed] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate target from features \n",
    "\n",
    "X = dfa.drop(columns=['rating_overall'])\n",
    "y = df['rating_overall']\n",
    "\n",
    "# splitting before vectorizing:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11940, 36)\n",
      "(2986, 36)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape) \n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(2, 3),  # include uni and 3-rams only\n",
    "                            max_df=.999, min_df=.007) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data_train = vectorizer.fit_transform(X_train['stem_str'])\n",
    "tfidf_data_test = vectorizer.transform(X_test['stem_str'])\n",
    "# tfidf_data_test.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11940, 188)\n",
      "(2986, 188)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_data_train.shape) \n",
    "print(tfidf_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['stem_str'])\n",
    "X_test = X_test.drop(columns=['stem_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11940 entries, 6078 to 5327\n",
      "Data columns (total 35 columns):\n",
      "employee_title_title_known               11940 non-null int64\n",
      "employee_title_title_unknown             11940 non-null int64\n",
      "employee_status_current_employee         11940 non-null int64\n",
      "employee_status_former_employee          11940 non-null int64\n",
      "employee_status_unknown_status           11940 non-null int64\n",
      "helpful_helpful                          11940 non-null int64\n",
      "helpful_not_helpful                      11940 non-null int64\n",
      "recommends_does_not_recommend            11940 non-null int64\n",
      "recommends_recommends                    11940 non-null int64\n",
      "recommends_unknown_recommends            11940 non-null int64\n",
      "area_central                             11940 non-null int64\n",
      "area_east_coast                          11940 non-null int64\n",
      "area_mountain                            11940 non-null int64\n",
      "area_outside_US                          11940 non-null int64\n",
      "area_unknown                             11940 non-null int64\n",
      "area_west_coast                          11940 non-null int64\n",
      "company_outlook_negative_outlook         11940 non-null int64\n",
      "company_outlook_neutral_outlook          11940 non-null int64\n",
      "company_outlook_outlook_unknown          11940 non-null int64\n",
      "company_outlook_positive_outlook         11940 non-null int64\n",
      "opinion_of_CEO_approves_of_CEO           11940 non-null int64\n",
      "opinion_of_CEO_disapproves_of_CEO        11940 non-null int64\n",
      "opinion_of_CEO_no_opinion_of_CEO         11940 non-null int64\n",
      "opinion_of_CEO_unknown_opinion_of_CEO    11940 non-null int64\n",
      "noun                                     11940 non-null float64\n",
      "adj                                      11940 non-null float64\n",
      "adv                                      11940 non-null float64\n",
      "verb                                     11940 non-null float64\n",
      "characters                               11940 non-null int64\n",
      "tokens                                   11940 non-null int64\n",
      "words                                    11940 non-null int64\n",
      "sentences                                11940 non-null int64\n",
      "avg_word_len                             11940 non-null float64\n",
      "avg_sent_len                             11940 non-null float64\n",
      "sentiment                                11940 non-null float64\n",
      "dtypes: float64(7), int64(28)\n",
      "memory usage: 3.3 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Non-Zero Elements in Vectorized Articles: 3.129899497487437\n",
      "Percentage of columns containing 0: 0.98335159841762\n"
     ]
    }
   ],
   "source": [
    "non_zero_cols = tfidf_data_train.nnz / float(tfidf_data_train.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(tfidf_data_train.shape[1]))\n",
    "print('Percentage of columns containing 0: {}'.format(percent_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tfidf to array and concatenate train and test sets\n",
    "\n",
    "X_train = pd.concat([pd.DataFrame(tfidf_data_train.toarray()), X_train.reset_index()], axis = 1)\n",
    "X_test = pd.concat([pd.DataFrame(tfidf_data_test.toarray()), X_test.reset_index()], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         6078\n",
       "1        10361\n",
       "2        10669\n",
       "3         8475\n",
       "4         1716\n",
       "         ...  \n",
       "11935    14704\n",
       "11936       48\n",
       "11937     8964\n",
       "11938     5944\n",
       "11939     5327\n",
       "Name: index, Length: 11940, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=32, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=32)\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_preds = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with Stematized Features:\n",
      "---------------\n",
      "Recall: 0.6072\n",
      "Testing Accuracy: 0.6072\n",
      "F1 Score: 0.5894\n"
     ]
    }
   ],
   "source": [
    "rf_recall = recall_score(y_test, rf_test_preds, average='weighted')\n",
    "rf_acc_score = accuracy_score(y_test, rf_test_preds)\n",
    "rf_f1_score = f1_score(y_test, rf_test_preds, average='weighted')\n",
    "print('Random Forest with Stematized Features:')\n",
    "print(3*'-----')\n",
    "\n",
    "print('Recall: {:.4}'.format(rf_recall))\n",
    "print(\"Testing Accuracy: {:.4}\".format(rf_acc_score))\n",
    "print(\"F1 Score: {:.4}\".format(rf_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='multi:softprob', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test_preds = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOOST with Stematized Features:\n",
      "---------------\n",
      "Recall: 0.6125\n",
      "Testing Accuracy: 0.6125\n",
      "F1 Score: 0.5968\n"
     ]
    }
   ],
   "source": [
    "xgb_recall = recall_score(y_test, xgb_test_preds, average='weighted')\n",
    "xgb_acc_score = accuracy_score(y_test, xgb_test_preds)\n",
    "xgb_f1_score = f1_score(y_test, xgb_test_preds, average='weighted')\n",
    "\n",
    "print('XGBOOST with Stematized Features:')\n",
    "print(3*'-----')\n",
    "\n",
    "print('Recall: {:.4}'.format(xgb_recall))\n",
    "\n",
    "print(\"Testing Accuracy: {:.4}\".format(xgb_acc_score))\n",
    "print(\"F1 Score: {:.4}\".format(xgb_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale X_train and X_test for svm\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clsf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modifier (text):\n",
    "    if '1' in text: return 1\n",
    "    if '2' in text: return 2\n",
    "    if '3' in text: return 3\n",
    "    if '4' in text: return 4\n",
    "    if '5' in text: return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'float' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-a7730b4a4ec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# y_train = y_train.map(modifier)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/ji/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   3826\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3827\u001b[0m         \"\"\"\n\u001b[0;32m-> 3828\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3829\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ji/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-153-2342133c1872>\u001b[0m in \u001b[0;36mmodifier\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodifier\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m'1'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'2'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'3'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'4'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'float' is not iterable"
     ]
    }
   ],
   "source": [
    "# y_train = y_train.map(modifier)\n",
    "y_test = y_test.map(modifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clsf.fit(scaled_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_test_preds = svm_clsf.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with Stematized Features:\n",
      "---------------\n",
      "Recall: 0.5633\n",
      "Testing Accuracy: 0.5633\n",
      "F1 Score: 0.5381\n"
     ]
    }
   ],
   "source": [
    "svm_recall = recall_score(y_test, svm_test_preds, average='weighted')\n",
    "svm_acc_score = accuracy_score(y_test, svm_test_preds)\n",
    "svm_f1_score = f1_score(y_test, svm_test_preds, average='weighted')\n",
    "print('Random Forest with Stematized Features:')\n",
    "print(3*'-----')\n",
    "\n",
    "print('Recall: {:.4}'.format(svm_recall))\n",
    "\n",
    "print(\"Testing Accuracy: {:.4}\".format(svm_acc_score))\n",
    "print(\"F1 Score: {:.4}\".format(svm_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\n",
    "model_log = logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_test_pred = model_log.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with Stematized Features:\n",
      "---------------\n",
      "Recall: 0.5727\n",
      "Testing Accuracy: 0.5727\n",
      "F1 Score: 0.5458\n"
     ]
    }
   ],
   "source": [
    "log_recall = recall_score(y_test, log_test_pred, average='weighted')\n",
    "log_acc_score = accuracy_score(y_test, log_test_pred)\n",
    "log_f1_score = f1_score(y_test, log_test_pred, average='weighted')\n",
    "print('Random Forest with Stematized Features:')\n",
    "print(3*'-----')\n",
    "\n",
    "print('Recall: {:.4}'.format(log_recall))\n",
    "\n",
    "print(\"Testing Accuracy: {:.4}\".format(log_acc_score))\n",
    "print(\"F1 Score: {:.4}\".format(log_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
